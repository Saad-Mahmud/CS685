{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f171740-86f4-4348-b3b6-2162f44ad45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "import pickle\n",
    "from datasets import Dataset\n",
    "from transformers import default_data_collator\n",
    "from tqdm import tqdm\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "803c945a-f261-4925-b9b9-c31d681b0333",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6eb32db-a308-4a1c-b908-7584953daa69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "login(token = \"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f0cce57-cf7d-490a-9963-7f90c28a46c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_pickle(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data['texts'],data['A'],data['Q']\n",
    "\n",
    "def custom_data_collator(features):\n",
    "    batch = default_data_collator(features)\n",
    "    input_ids = [f['input_ids'] for f in features]\n",
    "    input_ids = tokenizer(input_ids, padding=True, return_tensors=\"pt\")\n",
    "    batch['input_ids'] = input_ids['input_ids']\n",
    "    batch['labels'] =input_ids['input_ids'] # labels same as input_ids for LM\n",
    "    #batch['labels'][:,:20] = -100\n",
    "    return batch\n",
    "\n",
    "def prepare_dataset(texts,A,Q):\n",
    "    train_size = int(0.8 * len(texts))\n",
    "    train_set = Dataset.from_dict({\n",
    "        'input_ids': texts[:train_size],\n",
    "        'A': A[:train_size],\n",
    "        'Q': Q[:train_size]\n",
    "    })\n",
    "    test_set = Dataset.from_dict({\n",
    "        'input_ids': texts[train_size:],\n",
    "        'A': A[train_size:],\n",
    "        'Q': Q[train_size:]\n",
    "    })\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d2475ae-7e1d-4db9-a275-434b765dd4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "\n",
    "def prsanswer(text):\n",
    "        # Using regex to find the content inside the <ans> tags\n",
    "    match = re.search(r'<ans>\\s*(.*?)\\s*</ans>', text)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "\n",
    "    return None  # In case there is no match\n",
    "\n",
    "def calculate_accuracy(dataset, model, n=200):\n",
    "    corr = 0\n",
    "    tot = 0\n",
    "    for i in tqdm(range(0, min(len(dataset),n))):\n",
    "        tot+=1\n",
    "        input_ids = [dataset[i]['Q']+' <ans>']\n",
    "        input_ids = tokenizer(input_ids, return_tensors=\"pt\")\n",
    "        predictions = model.generate(input_ids=input_ids['input_ids'].to('cuda:0'), attention_mask=input_ids['attention_mask'].to('cuda:0'), max_length=128)\n",
    "        output = tokenizer.decode(predictions[0].tolist(), skip_special_tokens=True)\n",
    "        #print(output)\n",
    "        output = prsanswer(output)\n",
    "        \n",
    "        corr += (output == dataset[i]['A'])\n",
    "        #print(output,dataset[i]['A'],output == dataset[i]['A'])\n",
    "    print(corr/tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5bd63d8-855b-423b-bc53-002d2725e06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_save_push(model,lr, epoch, tokenizer, train, test, ood, target_modules, output_dir, hh):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=epoch,\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps = 2,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=100,\n",
    "        learning_rate=lr,\n",
    "        do_eval=True,\n",
    "    )\n",
    "    lora_config =  LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=target_modules,\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "            bias=\"none\"\n",
    "    )\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        peft_config=lora_config,\n",
    "        train_dataset=train_dataset,\n",
    "        dataset_text_field=\"input_ids\",\n",
    "        data_collator= custom_data_collator,\n",
    "        eval_dataset=test_dataset,\n",
    "        packing=True,\n",
    "        max_seq_length = 128\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model(f'{output_dir}/final')\n",
    "    model = PeftModel.from_pretrained(model, f'{output_dir}/final')\n",
    "    model.push_to_hub(f'saaduddinM/{hh}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2109b573-85f0-475e-84a4-8306d4ba40e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TMA = [[\"x_proj\", \"embeddings\", \"in_proj\", \"out_proj\"], [\"q_proj\", \"up_pro_proj\", \"k_proj\", \"down_proj\", \"v_proj\"],\n",
    "      [\"q_proj\", \"o_proj\", \"k_proj\",\"v_proj\"]]\n",
    "\n",
    "output_dir = \"Mamba1.4B/add_small\"\n",
    "model_name = \"state-spaces/mamba-1.4b-hf\"\n",
    "DS = 'dataset/addition_dataset_text_Small'\n",
    "hh = 'Mamba1.4B_add_small'\n",
    "lr =2e-3\n",
    "epoch = 7\n",
    "TM = 0\n",
    "\n",
    "\n",
    "output_dir = \"RGemma2B/add_small\"\n",
    "model_name = \"google/recurrentgemma-2b\"\n",
    "DS = 'dataset/addition_dataset_text_Small'\n",
    "hh = 'RGemma2B_add_small'\n",
    "lr =2e-4\n",
    "epoch = 7\n",
    "TM = 2\n",
    "\n",
    "\n",
    "output_dir = \"Zephyr3B/add_small\"\n",
    "model_name = \"stabilityai/stablelm-zephyr-3b\"\n",
    "DS = 'dataset/addition_dataset_text_Small'\n",
    "hh = 'Zephyr3B_add_small'\n",
    "lr =2e-4\n",
    "epoch = 7\n",
    "TM = 1\n",
    "\n",
    "output_dir = \"Phi1.5B/add_small\"\n",
    "model_name = \"microsoft/phi-1_5\"\n",
    "DS = 'dataset/addition_dataset_text_Small'\n",
    "hh = 'Phi1.5B_add_small'\n",
    "lr =2e-4\n",
    "epoch = 7\n",
    "TM = 1\n",
    "\n",
    "\n",
    "\n",
    "output_dir = \"Llama7B/add_small\"\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "DS = 'dataset/addition_dataset_text_Small'\n",
    "hh = 'Llama7B_add_small'\n",
    "lr =2e-4\n",
    "epoch = 7\n",
    "TM = 0\n",
    "\n",
    "output_dir = \"Mistral7B/add_small\"\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "DS = 'dataset/addition_dataset_text_Small'\n",
    "hh = 'Mistral7B_add_small'\n",
    "lr =2e-4\n",
    "epoch = 7\n",
    "TM = 0\n",
    "\n",
    "\n",
    "output_dir = \"Mamba2.8B/add_small\"\n",
    "model_name = \"state-spaces/mamba-2.8b-hf\"\n",
    "DS = 'dataset/addition_dataset_text_Small'\n",
    "hh = 'Mamba2.8B_add_small'\n",
    "lr =2e-4\n",
    "epoch = 7\n",
    "TM = 0\n",
    "\n",
    "output_dir = \"Gemma2B/add_small\"\n",
    "model_name = \"google/gemma-2b\"\n",
    "DS = 'dataset/addition_dataset_text_Small'\n",
    "hh = 'Gemma2B_add_small'\n",
    "lr =2e-4\n",
    "epoch = 7\n",
    "TM = 1\n",
    "\n",
    "\n",
    "output_dir = \"Llama8B/add_small\"\n",
    "model_name = \"meta-llama/Llama-3-8b-hf\"\n",
    "DS = 'dataset/addition_dataset_text_Small'\n",
    "hh = 'Llama8B_add_small'\n",
    "lr =2e-4\n",
    "epoch = 7\n",
    "TM = 1\n",
    "\n",
    "output_dir = \"Mistral7B/add_small\"\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "DS = 'dataset/addition_dataset_text_Small'\n",
    "hh = 'Mistral7B_add_small'\n",
    "lr =2e-4\n",
    "epoch = 7\n",
    "TM = 1\n",
    "\n",
    "output_dir = \"Llama8B/add_small\"\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "DS = 'dataset/addition_dataset_text_Small'\n",
    "hh = 'Llama8B_add_small'\n",
    "lr =2e-4\n",
    "epoch = 7\n",
    "TM = 1\n",
    "\n",
    "output_dir = \"Mamba7B/add_small\"\n",
    "model_name = \"tri-ml/mamba-7b-rw\"\n",
    "DS = 'dataset/addition_dataset_text_Small'\n",
    "hh = 'Mamba7B_add_small'\n",
    "lr =2e-4\n",
    "epoch = 7\n",
    "TM = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3d4ef5b-cc5e-46b5-8ee6-8cf86a0694b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0d86e1fa7345ee9a40445384effe4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer.torch_dtype = \"bfloat16\"\n",
    "model.torch_dtype = \"bfloat16\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "#model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "texts,A,Q = load_data_from_pickle(f'{DS}.pkl')\n",
    "train_dataset, test_dataset = prepare_dataset(texts,A,Q)\n",
    "texts,A,Q = load_data_from_pickle(f'{DS}_OOD.pkl')\n",
    "ood_test_dataset, _ = prepare_dataset(texts,A,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f9247a-be98-43eb-93ec-48035d81f360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='3500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  64/3500 00:17 < 15:45, 3.64 it/s, Epoch 0.13/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_save_push(model, lr, epoch, tokenizer, train_dataset, test_dataset, ood_test_dataset, TMA[TM], output_dir, hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ca9f143-26ff-4140-8936-0acf886cb496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at tri-ml/mamba-7b-rw were not used when initializing MambaForCausalLM: ['model.lm_head.weight']\n",
      "- This IS expected if you are initializing MambaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MambaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model = AutoModelForCausalLM.from_pretrained(f'saaduddinM/{hh}', torch_dtype=torch.bfloat16)\n",
    "texts,A,Q = load_data_from_pickle(f'{DS}.pkl')\n",
    "train_dataset, test_dataset = prepare_dataset(texts,A,Q)\n",
    "texts,A,Q = load_data_from_pickle(f'{DS}_OOD.pkl')\n",
    "ood_test_dataset, _ = prepare_dataset(texts,A,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc34f83b-ac5f-479e-bc6a-ad5c56f3cd98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaForCausalLM(\n",
       "  (backbone): MambaModel(\n",
       "    (embeddings): lora.Embedding(\n",
       "      (base_layer): Embedding(50432, 4096)\n",
       "      (lora_dropout): ModuleDict(\n",
       "        (default): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (lora_A): ModuleDict()\n",
       "      (lora_B): ModuleDict()\n",
       "      (lora_embedding_A): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 8x50432 (cuda:0)])\n",
       "      (lora_embedding_B): ParameterDict(  (default): Parameter containing: [torch.cuda.BFloat16Tensor of size 4096x8 (cuda:0)])\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-63): 64 x MambaBlock(\n",
       "        (norm): MambaRMSNorm()\n",
       "        (mixer): MambaMixer(\n",
       "          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192)\n",
       "          (act): SiLU()\n",
       "          (in_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=16384, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (x_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=8192, out_features=288, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=288, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (dt_proj): Linear(in_features=256, out_features=8192, bias=True)\n",
       "          (out_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=8192, out_features=4096, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_f): MambaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=50432, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746d08ac-656d-45c7-9979-2c6453e1c052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 47/200 [00:27<01:26,  1.77it/s]"
     ]
    }
   ],
   "source": [
    "calculate_accuracy(train_dataset, model)\n",
    "calculate_accuracy(test_dataset, model)\n",
    "calculate_accuracy(ood_test_dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259c439b-889f-43bc-bc99-b95c66d83c62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
